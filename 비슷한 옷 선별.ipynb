{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mkmy7/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-5-27 Python-3.11.9 torch-2.3.0 CUDA:0 (NVIDIA GeForce GTX 1050, 2048MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# 환경 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# YOLOv5 모델 로드\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "yolo_model.to(device).eval()\n",
    "\n",
    "# ResNet 모델 로드 및 특징 추출 레이어 설정\n",
    "resnet_model = resnet50(pretrained=True)\n",
    "resnet_model = torch.nn.Sequential(*list(resnet_model.children())[:-1])  # 마지막 FC layer 제거\n",
    "resnet_model.to(device).eval()\n",
    "\n",
    "# 변환 함수 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def extract_features(image, model):\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model(image).cpu().numpy().flatten()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 로드 함수 추가\n",
    "def load_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        return image\n",
    "    except IOError:\n",
    "        return None\n",
    "\n",
    "# 이미지에서 사람 탐지 및 상의/하의 추출\n",
    "def detect_and_extract(image_path):\n",
    "    image = load_image(image_path)\n",
    "    if image is None:\n",
    "        print(f\"이미지 로드 실패: {image_path}\")\n",
    "        return []\n",
    "\n",
    "    image_np = np.array(image)\n",
    "    results = yolo_model(image_np)\n",
    "    \n",
    "    clothes_images = []\n",
    "    \n",
    "    for detection in results.xyxy[0]:\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        if cls == 0:  # 사람 클래스\n",
    "            person_image = image.crop((int(x1), int(y1), int(x2), int(y2)))\n",
    "            height = int(y2) - int(y1)\n",
    "            top_image = person_image.crop((0, 0, person_image.width, height // 2))\n",
    "            bottom_image = person_image.crop((0, height // 2, person_image.width, height))\n",
    "            clothes_images.append((top_image, 'top'))\n",
    "            clothes_images.append((bottom_image, 'bottom'))\n",
    "    \n",
    "    return clothes_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디렉토리에서 옷 이미지 불러오기 및 특징 추출\n",
    "def load_clothes_images(directory):\n",
    "    clothes_features = []\n",
    "    clothes_paths = []\n",
    "    \n",
    "    files = os.listdir(directory)\n",
    "    print(f\"디렉토리 내 파일 목록: {files}\")\n",
    "    \n",
    "    for filename in files:\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_path = os.path.join(directory, filename)\n",
    "            image = load_image(image_path)\n",
    "            if image is None:\n",
    "                print(f\"이미지 로드 실패: {image_path}\")\n",
    "                continue\n",
    "            features = extract_features(image, resnet_model)\n",
    "            if features is None:\n",
    "                print(f\"특징 추출 실패: {image_path}\")\n",
    "                continue\n",
    "            clothes_features.append(features)\n",
    "            clothes_paths.append(image_path)\n",
    "    \n",
    "    return clothes_features, clothes_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 계산 및 top 3 유사한 옷 찾기\n",
    "def find_similar_clothes(clothes_images, clothes_features, clothes_paths):\n",
    "    similar_clothes = []\n",
    "\n",
    "    for clothes_image, clothes_type in clothes_images:\n",
    "        query_features = extract_features(clothes_image, resnet_model)\n",
    "        \n",
    "        if query_features is None:\n",
    "            print(f\"특징 추출 실패: {clothes_image}\")\n",
    "            continue\n",
    "        \n",
    "        query_features = query_features.reshape(1, -1)\n",
    "        \n",
    "        if len(clothes_features) == 0:\n",
    "            raise ValueError(\"clothes_features 배열이 비어 있습니다. 옷 이미지 디렉토리를 확인하세요.\")\n",
    "        \n",
    "        similarities = cosine_similarity(query_features, clothes_features)[0]\n",
    "        top_indices = similarities.argsort()[-3:][::-1]\n",
    "\n",
    "        for idx in top_indices:\n",
    "            similar_clothes.append((clothes_paths[idx], similarities[idx], clothes_type))\n",
    "\n",
    "    return similar_clothes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디렉토리 내 파일 목록: ['image_1.JPG', 'image_10.JPG', 'image_2.JPG', 'image_3.JPG', 'image_4.JPG', 'image_5.JPG', 'image_6.JPG', 'image_7.JPG', 'image_8.JPG', 'image_9.JPG']\n",
      "옷 이미지 디렉토리에서 이미지를 로드하지 못했습니다. 경로를 확인하세요.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be Tensor or ndarray. Got <class 'PIL.Image.Image'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(clothes_features) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m옷 이미지 디렉토리에서 이미지를 로드하지 못했습니다. 경로를 확인하세요.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m similar_clothes \u001b[38;5;241m=\u001b[39m \u001b[43mfind_similar_clothes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclothes_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclothes_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclothes_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path, similarity, clothes_type \u001b[38;5;129;01min\u001b[39;00m similar_clothes:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m유사한 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclothes_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (유사도: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[96], line 6\u001b[0m, in \u001b[0;36mfind_similar_clothes\u001b[1;34m(clothes_images, clothes_features, clothes_paths)\u001b[0m\n\u001b[0;32m      3\u001b[0m similar_clothes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clothes_image, clothes_type \u001b[38;5;129;01min\u001b[39;00m clothes_images:\n\u001b[1;32m----> 6\u001b[0m     query_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclothes_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresnet_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m특징 추출 실패: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclothes_image\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[93], line 22\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(image, model)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(image, model):\n\u001b[1;32m---> 22\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     24\u001b[0m         features \u001b[38;5;241m=\u001b[39m model(image)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[1;32mc:\\Users\\mkmy7\\anaconda3\\envs\\Pytorch\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\mkmy7\\anaconda3\\envs\\Pytorch\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:234\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mkmy7\\anaconda3\\envs\\Pytorch\\Lib\\site-packages\\torchvision\\transforms\\functional.py:268\u001b[0m, in \u001b[0;36mto_pil_image\u001b[1;34m(pic, mode)\u001b[0m\n\u001b[0;32m    266\u001b[0m     pic \u001b[38;5;241m=\u001b[39m pic\u001b[38;5;241m.\u001b[39mnumpy(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pic, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be Tensor or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# if 2D image, add channel dimension (HWC)\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     pic \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(pic, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: pic should be Tensor or ndarray. Got <class 'PIL.Image.Image'>."
     ]
    }
   ],
   "source": [
    "# 예제 실행 코드\n",
    "image_path = 'D:/minkwan/무신사 크롤링/coordikitty-ML-DL/착용 이미지/image_1.JPG'\n",
    "clothes_directory = 'D:/minkwan/무신사 크롤링/coordikitty-ML-DL/비슷한 이미지 선별 모음'\n",
    "\n",
    "clothes_images = detect_and_extract(image_path)\n",
    "clothes_features, clothes_paths = load_clothes_images(clothes_directory)\n",
    "\n",
    "if len(clothes_features) == 0:\n",
    "    print(\"옷 이미지 디렉토리에서 이미지를 로드하지 못했습니다. 경로를 확인하세요.\")\n",
    "\n",
    "similar_clothes = find_similar_clothes(clothes_images, clothes_features, clothes_paths)\n",
    "\n",
    "for path, similarity, clothes_type in similar_clothes:\n",
    "    print(f\"유사한 {clothes_type}: {path} (유사도: {similarity})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
